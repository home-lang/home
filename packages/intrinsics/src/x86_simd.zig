// Home Programming Language - x86 SIMD Intrinsics
// Comprehensive SSE, AVX, and AVX-512 support

const std = @import("std");
const builtin = @import("builtin");

/// FPU control and status
pub const FPU = struct {
    /// FPU control word register
    pub const ControlWord = packed struct {
        invalid_operation_mask: bool,
        denormal_operand_mask: bool,
        zero_divide_mask: bool,
        overflow_mask: bool,
        underflow_mask: bool,
        precision_mask: bool,
        reserved1: u2,
        precision_control: u2, // 00=24bit, 01=reserved, 10=53bit, 11=64bit
        rounding_control: u2, // 00=nearest, 01=down, 10=up, 11=truncate
        infinity_control: bool,
        reserved2: u3,
    };

    /// Get FPU control word
    pub fn getControl() u16 {
        if (!comptime isX86()) return 0;
        var cw: u16 = undefined;
        asm volatile ("fnstcw %[cw]"
            : [cw] "=m" (cw),
        );
        return cw;
    }

    /// Set FPU control word
    pub fn setControl(cw: u16) void {
        if (!comptime isX86()) return;
        asm volatile ("fldcw %[cw]"
            :
            : [cw] "m" (cw),
        );
    }

    /// Get FPU status word
    pub fn getStatus() u16 {
        if (!comptime isX86()) return 0;
        var sw: u16 = undefined;
        asm volatile ("fnstsw %[sw]"
            : [sw] "=m" (sw),
        );
        return sw;
    }

    /// Clear exceptions
    pub fn clearExceptions() void {
        if (!comptime isX86()) return;
        asm volatile ("fnclex");
    }

    /// Initialize FPU
    pub fn init() void {
        if (!comptime isX86()) return;
        asm volatile ("finit");
    }
};

/// SSE intrinsics
pub const SSE = struct {
    /// Check if SSE is available
    pub fn isAvailable() bool {
        return switch (builtin.cpu.arch) {
            .x86_64 => true, // Always available on x86_64
            .x86 => std.Target.x86.featureSetHas(builtin.cpu.features, .sse),
            else => false,
        };
    }

    /// Load 128 bits (must be 16-byte aligned)
    pub fn load(comptime T: type, ptr: [*]const T) @Vector(16 / @sizeOf(T), T) {
        if (!comptime isX86()) @compileError("SSE only available on x86");
        return @as(*align(16) const @Vector(16 / @sizeOf(T), T), @ptrCast(ptr)).*;
    }

    /// Store 128 bits (must be 16-byte aligned)
    pub fn store(comptime T: type, ptr: [*]T, value: @Vector(16 / @sizeOf(T), T)) void {
        if (!comptime isX86()) @compileError("SSE only available on x86");
        @as(*align(16) @Vector(16 / @sizeOf(T), T), @ptrCast(ptr)).* = value;
    }

    /// Load 128 bits (unaligned)
    pub fn loadu(comptime T: type, ptr: [*]const T) @Vector(16 / @sizeOf(T), T) {
        if (!comptime isX86()) @compileError("SSE only available on x86");
        return @as(*const @Vector(16 / @sizeOf(T), T), @ptrCast(ptr)).*;
    }

    /// Store 128 bits (unaligned)
    pub fn storeu(comptime T: type, ptr: [*]T, value: @Vector(16 / @sizeOf(T), T)) void {
        if (!comptime isX86()) @compileError("SSE only available on x86");
        @as(*@Vector(16 / @sizeOf(T), T), @ptrCast(ptr)).* = value;
    }

    /// Reciprocal estimate (1/x)
    pub fn rcp_ps(a: @Vector(4, f32)) @Vector(4, f32) {
        if (!comptime isX86()) @compileError("SSE only available on x86");
        // Zig doesn't have direct intrinsic, use division approximation
        const ones = @Vector(4, f32){ 1.0, 1.0, 1.0, 1.0 };
        return ones / a;
    }

    /// Reciprocal square root estimate (1/sqrt(x))
    pub fn rsqrt_ps(a: @Vector(4, f32)) @Vector(4, f32) {
        if (!comptime isX86()) @compileError("SSE only available on x86");
        const ones = @Vector(4, f32){ 1.0, 1.0, 1.0, 1.0 };
        return ones / @sqrt(a);
    }

    /// Compare equal
    pub fn cmpeq_ps(a: @Vector(4, f32), b: @Vector(4, f32)) @Vector(4, u32) {
        const mask = a == b;
        return @select(u32, mask, @Vector(4, u32){ 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF }, @Vector(4, u32){ 0, 0, 0, 0 });
    }

    /// Compare less than
    pub fn cmplt_ps(a: @Vector(4, f32), b: @Vector(4, f32)) @Vector(4, u32) {
        const mask = a < b;
        return @select(u32, mask, @Vector(4, u32){ 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF }, @Vector(4, u32){ 0, 0, 0, 0 });
    }

    /// Compare less than or equal
    pub fn cmple_ps(a: @Vector(4, f32), b: @Vector(4, f32)) @Vector(4, u32) {
        const mask = a <= b;
        return @select(u32, mask, @Vector(4, u32){ 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF }, @Vector(4, u32){ 0, 0, 0, 0 });
    }

    /// Bitwise AND
    pub fn and_ps(a: @Vector(4, f32), b: @Vector(4, f32)) @Vector(4, f32) {
        const a_bits: @Vector(4, u32) = @bitCast(a);
        const b_bits: @Vector(4, u32) = @bitCast(b);
        return @bitCast(a_bits & b_bits);
    }

    /// Bitwise OR
    pub fn or_ps(a: @Vector(4, f32), b: @Vector(4, f32)) @Vector(4, f32) {
        const a_bits: @Vector(4, u32) = @bitCast(a);
        const b_bits: @Vector(4, u32) = @bitCast(b);
        return @bitCast(a_bits | b_bits);
    }

    /// Bitwise XOR
    pub fn xor_ps(a: @Vector(4, f32), b: @Vector(4, f32)) @Vector(4, f32) {
        const a_bits: @Vector(4, u32) = @bitCast(a);
        const b_bits: @Vector(4, u32) = @bitCast(b);
        return @bitCast(a_bits ^ b_bits);
    }
};

/// SSE2 intrinsics
pub const SSE2 = struct {
    pub fn isAvailable() bool {
        return switch (builtin.cpu.arch) {
            .x86_64 => true, // Always available on x86_64
            .x86 => std.Target.x86.featureSetHas(builtin.cpu.features, .sse2),
            else => false,
        };
    }

    /// Load 128 bits of integer data
    pub fn load_si128(ptr: [*]const u8) @Vector(16, u8) {
        if (!comptime isX86()) @compileError("SSE2 only available on x86");
        return @as(*align(16) const @Vector(16, u8), @ptrCast(ptr)).*;
    }

    /// Store 128 bits of integer data
    pub fn store_si128(ptr: [*]u8, value: @Vector(16, u8)) void {
        if (!comptime isX86()) @compileError("SSE2 only available on x86");
        @as(*align(16) @Vector(16, u8), @ptrCast(ptr)).* = value;
    }

    /// Pack 32-bit integers to 16-bit with unsigned saturation
    pub fn packus_epi32(a: @Vector(4, i32), b: @Vector(4, i32)) @Vector(8, u16) {
        var result: @Vector(8, u16) = undefined;
        inline for (0..4) |i| {
            result[i] = @intCast(@min(@max(a[i], 0), 65535));
            result[i + 4] = @intCast(@min(@max(b[i], 0), 65535));
        }
        return result;
    }

    /// Shuffle double-precision values
    pub fn shuffle_pd(a: @Vector(2, f64), b: @Vector(2, f64), comptime mask: u8) @Vector(2, f64) {
        const indices = @Vector(2, i32){
            if ((mask & 0x01) != 0) 1 else 0,
            if ((mask & 0x02) != 0) 3 else 2,
        };
        return @shuffle(f64, a, b, indices);
    }
};

/// AVX intrinsics
pub const AVX = struct {
    pub fn isAvailable() bool {
        return switch (builtin.cpu.arch) {
            .x86_64, .x86 => std.Target.x86.featureSetHas(builtin.cpu.features, .avx),
            else => false,
        };
    }

    /// Load 256 bits (must be 32-byte aligned)
    pub fn load_ps(ptr: [*]const f32) @Vector(8, f32) {
        if (!comptime isX86()) @compileError("AVX only available on x86");
        return @as(*align(32) const @Vector(8, f32), @ptrCast(ptr)).*;
    }

    /// Store 256 bits (must be 32-byte aligned)
    pub fn store_ps(ptr: [*]f32, value: @Vector(8, f32)) void {
        if (!comptime isX86()) @compileError("AVX only available on x86");
        @as(*align(32) @Vector(8, f32), @ptrCast(ptr)).* = value;
    }

    /// Load 256 bits (unaligned)
    pub fn loadu_ps(ptr: [*]const f32) @Vector(8, f32) {
        if (!comptime isX86()) @compileError("AVX only available on x86");
        return @as(*const @Vector(8, f32), @ptrCast(ptr)).*;
    }

    /// Store 256 bits (unaligned)
    pub fn storeu_ps(ptr: [*]f32, value: @Vector(8, f32)) void {
        if (!comptime isX86()) @compileError("AVX only available on x86");
        @as(*@Vector(8, f32), @ptrCast(ptr)).* = value;
    }

    /// Broadcast scalar to all elements
    pub fn broadcast_ss(ptr: *const f32) @Vector(8, f32) {
        return @splat(ptr.*);
    }

    /// Permute 256-bit value based on control
    pub fn permute_ps(a: @Vector(8, f32), comptime control: u8) @Vector(8, f32) {
        const mask = @Vector(8, i32){
            @as(i32, control & 0x3),
            @as(i32, (control >> 2) & 0x3),
            @as(i32, (control >> 4) & 0x3),
            @as(i32, (control >> 6) & 0x3),
            @as(i32, 4 + (control & 0x3)),
            @as(i32, 4 + ((control >> 2) & 0x3)),
            @as(i32, 4 + ((control >> 4) & 0x3)),
            @as(i32, 4 + ((control >> 6) & 0x3)),
        };
        return @shuffle(f32, a, undefined, mask);
    }

    /// Horizontal add
    pub fn hadd_ps(a: @Vector(8, f32), b: @Vector(8, f32)) @Vector(8, f32) {
        return @Vector(8, f32){
            a[0] + a[1],
            a[2] + a[3],
            b[0] + b[1],
            b[2] + b[3],
            a[4] + a[5],
            a[6] + a[7],
            b[4] + b[5],
            b[6] + b[7],
        };
    }
};

/// AVX2 intrinsics
pub const AVX2 = struct {
    pub fn isAvailable() bool {
        return switch (builtin.cpu.arch) {
            .x86_64, .x86 => std.Target.x86.featureSetHas(builtin.cpu.features, .avx2),
            else => false,
        };
    }

    /// Gather double-precision values
    pub fn gather_pd(
        base: [*]const f64,
        indices: @Vector(4, i32),
        comptime scale: i32,
    ) @Vector(4, f64) {
        var result: @Vector(4, f64) = undefined;
        inline for (0..4) |i| {
            const offset: isize = @as(isize, indices[i]) * scale;
            const ptr: [*]const f64 = @ptrFromInt(@intFromPtr(base) + @as(usize, @intCast(offset)));
            result[i] = ptr[0];
        }
        return result;
    }

    /// Permute 256-bit integers across lanes
    pub fn permute4x64_epi64(a: @Vector(4, i64), comptime control: u8) @Vector(4, i64) {
        const mask = @Vector(4, i32){
            @as(i32, control & 0x3),
            @as(i32, (control >> 2) & 0x3),
            @as(i32, (control >> 4) & 0x3),
            @as(i32, (control >> 6) & 0x3),
        };
        return @shuffle(i64, a, undefined, mask);
    }

    /// Blend 256-bit integers based on mask
    pub fn blend_epi32(a: @Vector(8, i32), b: @Vector(8, i32), comptime mask: u8) @Vector(8, i32) {
        const select_mask = @Vector(8, bool){
            (mask & 0x01) != 0,
            (mask & 0x02) != 0,
            (mask & 0x04) != 0,
            (mask & 0x08) != 0,
            (mask & 0x10) != 0,
            (mask & 0x20) != 0,
            (mask & 0x40) != 0,
            (mask & 0x80) != 0,
        };
        return @select(i32, select_mask, b, a);
    }
};

/// FMA (Fused Multiply-Add) intrinsics
pub const FMA = struct {
    pub fn isAvailable() bool {
        return switch (builtin.cpu.arch) {
            .x86_64, .x86 => std.Target.x86.featureSetHas(builtin.cpu.features, .fma),
            else => false,
        };
    }

    /// Fused multiply-add: a * b + c (128-bit)
    pub fn fmadd_ps(a: @Vector(4, f32), b: @Vector(4, f32), c: @Vector(4, f32)) @Vector(4, f32) {
        return @mulAdd(@Vector(4, f32), a, b, c);
    }

    /// Fused multiply-subtract: a * b - c (128-bit)
    pub fn fmsub_ps(a: @Vector(4, f32), b: @Vector(4, f32), c: @Vector(4, f32)) @Vector(4, f32) {
        return @mulAdd(@Vector(4, f32), a, b, -c);
    }

    /// Fused negative multiply-add: -(a * b) + c (128-bit)
    pub fn fnmadd_ps(a: @Vector(4, f32), b: @Vector(4, f32), c: @Vector(4, f32)) @Vector(4, f32) {
        return @mulAdd(@Vector(4, f32), -a, b, c);
    }

    /// Fused multiply-add: a * b + c (256-bit)
    pub fn fmadd256_ps(a: @Vector(8, f32), b: @Vector(8, f32), c: @Vector(8, f32)) @Vector(8, f32) {
        return @mulAdd(@Vector(8, f32), a, b, c);
    }
};

fn isX86() bool {
    return switch (builtin.cpu.arch) {
        .x86_64, .x86 => true,
        else => false,
    };
}

// Tests
test "FPU control" {
    if (!comptime isX86()) return error.SkipZigTest;

    const old_cw = FPU.getControl();
    FPU.setControl(old_cw);
    const new_cw = FPU.getControl();
    const testing = std.testing;
    try testing.expectEqual(old_cw, new_cw);
}

test "SSE load/store" {
    if (!comptime isX86()) return error.SkipZigTest;
    if (!SSE.isAvailable()) return error.SkipZigTest;

    var data: [4]f32 align(16) = .{ 1.0, 2.0, 3.0, 4.0 };
    const vec = SSE.load(f32, &data);

    const testing = std.testing;
    try testing.expectEqual(@as(f32, 1.0), vec[0]);
    try testing.expectEqual(@as(f32, 2.0), vec[1]);
    try testing.expectEqual(@as(f32, 3.0), vec[2]);
    try testing.expectEqual(@as(f32, 4.0), vec[3]);
}

test "AVX operations" {
    if (!comptime isX86()) return error.SkipZigTest;
    if (!AVX.isAvailable()) return error.SkipZigTest;

    const a = @Vector(8, f32){ 1, 2, 3, 4, 5, 6, 7, 8 };
    const b = @Vector(8, f32){ 8, 7, 6, 5, 4, 3, 2, 1 };

    const result = AVX.hadd_ps(a, b);
    const testing = std.testing;
    try testing.expectEqual(@as(f32, 3.0), result[0]); // 1+2
    try testing.expectEqual(@as(f32, 7.0), result[1]); // 3+4
}

test "FMA operations" {
    if (!comptime isX86()) return error.SkipZigTest;
    if (!FMA.isAvailable()) return error.SkipZigTest;

    const a = @Vector(4, f32){ 2.0, 3.0, 4.0, 5.0 };
    const b = @Vector(4, f32){ 1.0, 2.0, 3.0, 4.0 };
    const c = @Vector(4, f32){ 1.0, 1.0, 1.0, 1.0 };

    const result = FMA.fmadd_ps(a, b, c);
    const testing = std.testing;
    try testing.expectEqual(@as(f32, 3.0), result[0]); // 2*1+1
    try testing.expectEqual(@as(f32, 7.0), result[1]); // 3*2+1
    try testing.expectEqual(@as(f32, 13.0), result[2]); // 4*3+1
    try testing.expectEqual(@as(f32, 21.0), result[3]); // 5*4+1
}
